{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azul2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.nn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mquantization\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch.nn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.quantization as quantization\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.quantized.supported_engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3682 + 8 - 3682 % 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AzulNnue(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AzulNnue, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(3688, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1080),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def get_move(self, game_state):\n",
    "        possible_moves, _ = game_state.get_possible_moves()\n",
    "        encoding = encode_game_state(game_state, int(game_state.current_player))\n",
    "        values = self.forward(torch.tensor(encoding, dtype=torch.float32).clone().detach()).squeeze(0).clone().detach()\n",
    "\n",
    "        max_value = float('-inf')\n",
    "        best_move = None\n",
    "        for move in possible_moves:\n",
    "            move_index = encode_move(game_state, move)\n",
    "            value = values[move_index].item()\n",
    "            if value > max_value:\n",
    "                max_value = value\n",
    "                best_move = move\n",
    "\n",
    "        return best_move\n",
    "\n",
    "model = AzulNnue()\n",
    "loss_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "loader = DataLoader(\"http://127.0.0.1:3044\", 64)\n",
    "loader.set_target_buffer_size(3000)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "print(\"Training started on\", device)\n",
    "step = 0\n",
    "try:\n",
    "    sum_loss = 0\n",
    "    for x, y, y_mask in loader:\n",
    "\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        y_mask = torch.tensor(y_mask, dtype=torch.float32)\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_mask = y_mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        masked_loss = loss * y_mask\n",
    "        loss = masked_loss.sum() / y_mask.sum()  \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step += 1\n",
    "        # loss_history.append(loss.item())\n",
    "        sum_loss += loss.item()\n",
    "        if step % 100 == 0:\n",
    "            loss_history.append(sum_loss / 100)\n",
    "            sum_loss = 0\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            clear_output(True)\n",
    "            plt.plot(loss_history, label='loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted\")\n",
    "    model.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AzulNnue()\n",
    "\n",
    "# model.qconfig = quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# # Prepare the model for static quantization\n",
    "# model_prepared = quantization.prepare(model)\n",
    "\n",
    "# # Example calibration data - this should be representative of the data the model will be used on\n",
    "# # For simplicity, we are creating random data. Replace this with actual game state encodings.\n",
    "# calibration_data = [torch.randn(1, 3688) for _ in range(100)]\n",
    "\n",
    "# # Calibrate the model\n",
    "# model_prepared.eval()\n",
    "# with torch.no_grad():\n",
    "#     for data in calibration_data:\n",
    "#         model_prepared(data)\n",
    "\n",
    "# # Convert the model to a quantized version\n",
    "# model_quantized = quantization.convert(model_prepared)\n",
    "\n",
    "# # orig = model(x)[0].detach().numpy()\n",
    "# # quant = model_quantized(x)[0].detach().numpy()\n",
    "# # orig\n",
    "\n",
    "# test_data = torch.randn(1, 3688)\n",
    "# output = model_quantized(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history[-1] # 0.0018468010996002704 0.010775361494161188 0.008451616615056992 0.025006712302565574 0.008634282290004194\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "model.to(\"cpu\")\n",
    "\n",
    "game_state = GameState()\n",
    "game_state.set_player_names([\"Neural network\", \"Random\"])\n",
    "while True:\n",
    "    possible_moves, state = game_state.get_possible_moves()\n",
    "    print(game_state)\n",
    "    if state.is_game_over():\n",
    "        break\n",
    "    if int(game_state.current_player) == 0:\n",
    "        move = model.get_move(game_state)\n",
    "    else:\n",
    "        move = random.choice(possible_moves)\n",
    "    print(move)\n",
    "    game_state.do_move(move)\n",
    "\n",
    "game_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "mcts = MonteCarloTreeSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 ** 5 * (5 * 2 + 1)\n",
    "game_state.fen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # game_state = GameState(\"2_1_0_12935366915_47345960200_0-0-0-0-0-100859904_65537000_3_14102-66445_4311876097-17230266369_4345298947-17180000001_1\")\n",
    "# # game_state = GameState(\"2_0_1_38739051528_4328850691_73744-135169-8194-131329-0-16842752_66126842_0_17078043-3498903_33554688-17180000256_1095250150655-17163223039_0\")\n",
    "# # game_state = GameState(\"2_0_1_60331723785_16843270_0-0-0-0-0-33555202_65668075_257_16897-268468232_12918456833-17180066304_12968722691-21458191103_1\")\n",
    "# print(game_state.fen)\n",
    "# print(game_state)\n",
    "# mcts.stop_working()\n",
    "# mcts.advance_root(game_state, None)\n",
    "# mcts.start_working()\n",
    "# time.sleep(0.7)\n",
    "# mcts.stop_working()\n",
    "# # mcts.reset()\n",
    "# mcts_value = mcts.value\n",
    "# value_network_evaluation = model.value_of(game_state).detach().item()\n",
    "# mcts_evaluation = mcts_value[0]\n",
    "\n",
    "# print(\"Value network evaluation:\", value_network_evaluation)\n",
    "# print(\"MCTS evaluation:\", mcts_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts.principal_variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encoding = encode_game_state(game_state)\n",
    "# # tensor_encoding = torch.tensor(encoding, dtype=torch.float32).clone().detach()\n",
    "# # x = tensor_encoding\n",
    "# import numpy as np\n",
    "# x = torch.tensor([0] * 1816, dtype=torch.float32)\n",
    "# x[0] = 1\n",
    "# print(\"Input: \", list(x.clone().detach().numpy()))\n",
    "# x = model.layers[0](x)\n",
    "# print(\"First layer output: \", list(x.clone().detach().numpy()))\n",
    "# x = model.layers[1](x)\n",
    "# print(\"First ReLU output: \", list(x.clone().detach().numpy()))\n",
    "# x = model.layers[2](x)\n",
    "# print(\"Second layer output: \", list(x.clone().detach().numpy()))\n",
    "# x = model.layers[3](x)\n",
    "# print(\"Second ReLU output: \", list(x.clone().detach().numpy()))\n",
    "# x = model.layers[4](x)\n",
    "# print(\"Third layer output: \", list(x.clone().detach().numpy()))\n",
    "# x = model.layers[5](x)\n",
    "# print(\"Sigmoid output: \", list(x.clone().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# state_dict = model.state_dict()\n",
    "# weights_biases = {k: v.cpu().numpy().tolist() for k, v in state_dict.items()}\n",
    "\n",
    "# with open('../logs/model_weights.json', 'w') as f:\n",
    "#     json.dump(weights_biases, f)\n",
    "\n",
    "# Store the weights binary\n",
    "# torch.save(model.state_dict(), '../logs/model_weights.pth')\n",
    "model.to(\"cpu\")\n",
    "# For each layer extract the weights and biases\n",
    "layers = []\n",
    "for layer in model.layers:\n",
    "    try:\n",
    "        weights = layer.weight.detach().numpy()\n",
    "        biases = layer.bias.detach().numpy()\n",
    "        layers.append(WeightsBiases(weights, biases))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# Store the weights and biases\n",
    "store_model('../logs/model_weights.bin', layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[0].get_parameter('weight')\n",
    "print(\"weights[0][0]:\", weights[0][0].item())\n",
    "print(\"weights[0][1]:\", weights[0][1].item())\n",
    "print(\"weights[1][0]:\", weights[1][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state = GameState(\"2_0_1_38739051528_4328850691_73744-135169-8194-131329-0-16842752_66126842_0_17078043-3498903_33554688-17180000256_1095250150655-17163223039_0\")\n",
    "print(game_state.fen)\n",
    "game_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encode_game_state(game_state, 0)\n",
    "model(torch.tensor(encoded, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = AzulNnue()\n",
    "\n",
    "# Quantization configuration\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Fuse layers (if needed, but in this simple case, we have no layers to fuse)\n",
    "# Fuse operations like Conv + BatchNorm + ReLU into a single operation\n",
    "# model = torch.quantization.fuse_modules(model, [['layer_name1', 'layer_name2', ...]])\n",
    "\n",
    "# Prepare the model for static quantization\n",
    "model = torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "# Calibrate the model with representative data\n",
    "# Here you would normally run a few batches of data through the model\n",
    "# For example, you could do something like this:\n",
    "# with torch.no_grad():\n",
    "#     for data in calibration_data_loader:\n",
    "#         model(data)\n",
    "\n",
    "# Convert the model to a quantized version\n",
    "quant_model = torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "# Example: Save the quantized model\n",
    "# torch.save(model.state_dict(), 'azul_nnue_quantized.pth')\n",
    "\n",
    "# Example: Load the quantized model\n",
    "# model = AzulNnue()\n",
    "# model.load_state_dict(torch.load('azul_nnue_quantized.pth'))\n",
    "\n",
    "# Now, the model is quantized and ready for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_quantized(model, input_data):\n",
    "    # Ensure input data is a tensor\n",
    "    if not isinstance(input_data, torch.Tensor):\n",
    "        input_data = torch.tensor(input_data, dtype=torch.float32)\n",
    "    # Perform feedforward\n",
    "    with torch.no_grad():\n",
    "        output = model(input_data)\n",
    "    return output\n",
    "\n",
    "# Example input data (replace with actual data)\n",
    "input_data = [0.5] * 3688  # Example input of correct size\n",
    "\n",
    "# Perform feedforward with the quantized model\n",
    "output = feed_forward_quantized(quant_model, input_data)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GameState(\"2_0_1_47362607365_25920996362_0-0-0-0-0-65792_65537001_3584_33413-1170_8640331777-17213620480_33750788-4328719615_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "71 * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "71 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
